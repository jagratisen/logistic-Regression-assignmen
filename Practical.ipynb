{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooccKJWSiqMp",
        "outputId": "9d7b9298-d01f-4093-bdb0-54cfc66794af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# 1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "#Regression, and prints the model accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "# and print the model accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L1 regularization\n",
        "# Note: 'liblinear' solver is required for L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with L1 Regularization:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fopz7NDjZ9b",
        "outputId": "32eb9bc6-58cd-4eee-e764-391e408f4773"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "#LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='auto')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with L2 Regularization:\", accuracy)\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Optional: Print model intercept\n",
        "print(\"\\nModel Intercept:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF-_39Fwj1W_",
        "outputId": "fda2460e-1b00-40d8-d42c-0ae9625d601d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.0\n",
            "\n",
            "Model Coefficients:\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "\n",
            "Model Intercept:\n",
            "[  9.00884295   1.86902164 -10.87786459]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with Elastic Net regularization\n",
        "# Note: Only 'saga' solver supports elasticnet penalty\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,           # Balance between L1 and L2 (0 = L2, 1 = L1)\n",
        "    max_iter=200,\n",
        "    multi_class='auto'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Elastic Net Regularization:\", accuracy)\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Step 7: Print model intercept\n",
        "print(\"\\nModel Intercept:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwkCeyaOkcZw",
        "outputId": "abf33349-fa76-4152-d6cb-6de614967469"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.0\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.38833257  1.77101015 -2.41962971 -0.71144716]\n",
            " [ 0.07815042  0.          0.         -0.58008749]\n",
            " [-1.2597022  -1.53181867  2.59560707  2.08256651]]\n",
            "\n",
            "Model Intercept:\n",
            "[ 1.21892996  1.57322054 -2.79215049]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "# multi_class='ovr'\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels (3 classes)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with One-vs-Rest (OvR):\", accuracy)\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Step 7: Print model intercept\n",
        "print(\"\\nModel Intercept:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0u7J5jrdz5c",
        "outputId": "a777c1df-8b53-4828-e97b-ab6f2f1114b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-Rest (OvR): 1.0\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n",
            " [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n",
            " [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n",
            "\n",
            "Model Intercept:\n",
            "[ 0.2478905   0.86408083 -1.00411267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "# Regression. Print the best parameters and accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Step 4: Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],               # Regularization strength\n",
        "    'penalty': ['l1', 'l2']                    # L1 (Lasso) or L2 (Ridge)\n",
        "}\n",
        "\n",
        "# Step 5: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get best parameters and evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPupiD_eGU_",
        "outputId": "836b1ebd-945c-4647-fee6-068c8a7efcf5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Test Set Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        " # average accuracy\n",
        " # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Define the model\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Step 3: Set up Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print accuracy for each fold and the average\n",
        "print(\"Accuracy for each fold:\", scores)\n",
        "print(\"Average Accuracy:\", np.mean(scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAohIoNZd756",
        "outputId": "07ba8200-fbdd-4ddb-fa73-9f6d9e10e8d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.96666667 1.         0.9        0.93333333 1.        ]\n",
            "Average Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "# accuracy.\n",
        "# 8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "# accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV file\n",
        "# Replace 'your_file.csv' with your actual file path\n",
        "# Make sure the file 'your_file.csv' exists in the correct directory,\n",
        "# or provide the full path to the file.\n",
        "try:\n",
        "    df = pd.read_csv('your_file.csv')\n",
        "\n",
        "    # Step 2: Separate features and target\n",
        "    # Replace 'target_column' with your actual target column name\n",
        "    # Ensure 'target_column' exists in your CSV file\n",
        "    if 'target_column' not in df.columns:\n",
        "        print(\"Error: 'target_column' not found in the CSV file.\")\n",
        "    else:\n",
        "        X = df.drop('target_column', axis=1)\n",
        "        y = df['target_column']\n",
        "\n",
        "        # Step 3: Split the dataset into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Step 4: Apply Logistic Regression\n",
        "        model = LogisticRegression(max_iter=200)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Step 5: Predict and evaluate\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # Step 6: Print accuracy\n",
        "        print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'your_file.csv' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb-WyydXev05",
        "outputId": "0e4bdf51-7d76-486b-d608-02ed57cbbfd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The file 'your_file.csv' was not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "# Logistic Regression. Print the best parameters and accuracy.\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model\n",
        "# Note: The solver 'saga' supports all penalties ('l1', 'l2', 'elasticnet', 'none')\n",
        "# We will use 'saga' here to allow searching over all penalty types.\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for 'saga' solver\n",
        "\n",
        "# Step 4: Define the hyperparameter distribution for RandomizedSearchCV\n",
        "# Using distributions from scipy.stats for sampling\n",
        "param_distributions = {\n",
        "    'C': uniform(loc=0, scale=100),      # Continuous distribution for C (regularization strength)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'], # Categorical distribution for penalty\n",
        "    'solver': ['saga'],                  # 'saga' supports elasticnet and none\n",
        "    'l1_ratio': uniform(loc=0, scale=1)  # Distribution for l1_ratio (only used with elasticnet)\n",
        "}\n",
        "\n",
        "# Step 5: Apply RandomizedSearchCV\n",
        "# n_iter: Number of parameter settings that are sampled.\n",
        "# cv: Number of folds for cross-validation.\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,           # Number of parameter settings to sample\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    random_state=42,     # for reproducibility\n",
        "    n_jobs=-1            # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit RandomizedSearchCV on the training data\n",
        "# RandomizedSearchCV handles the training within the cross-validation process\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get best parameters and evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Test Set Accuracy with Best Parameters:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijAzTVLrfQDz",
        "outputId": "fd0222e3-2239-4c81-8fe5-cd7deb8ec8bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': np.float64(33.37086111390219), 'l1_ratio': np.float64(0.14286681792194078), 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "Test Set Accuracy with Best Parameters: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "65 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "42 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "23 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.95833333 0.95833333        nan 0.96666667        nan        nan\n",
            " 0.96666667 0.96666667        nan 0.95833333 0.95833333 0.95833333\n",
            " 0.95833333 0.96666667 0.95833333        nan 0.96666667 0.96666667\n",
            " 0.95833333 0.95833333 0.95833333 0.95833333 0.95833333 0.95833333\n",
            " 0.95833333        nan 0.95833333 0.95833333        nan        nan\n",
            " 0.95833333        nan 0.95833333 0.96666667        nan 0.95833333\n",
            " 0.95833333 0.95833333 0.95833333 0.96666667        nan 0.95833333\n",
            " 0.96666667 0.95833333 0.96666667 0.95833333        nan        nan\n",
            " 0.95833333 0.95833333]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels (3 classes)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the base Logistic Regression model\n",
        "# This model will be used for each binary classification problem (one for each pair of classes)\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Step 4: Implement One-vs-One strategy\n",
        "# OneVsOneClassifier fits one classifier for each pair of classes.\n",
        "# The base_model is the classifier used for each pair.\n",
        "ovo_classifier = OneVsOneClassifier(base_model)\n",
        "\n",
        "# Step 5: Train the OvO classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on test data\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Step 7: Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with One-vs-One (OvO):\", accuracy)\n",
        "\n",
        "# Note: OneVsOneClassifier does not directly expose coefficients and intercept\n",
        "# in the same way as a single LogisticRegression model for all classes.\n",
        "# You would need to access the individual classifiers fitted for each pair of classes\n",
        "# if you wanted to inspect their parameters.\n",
        "# Example (optional - uncomment to see how to access individual classifiers):\n",
        "# print(\"\\nIndividual Binary Classifiers:\")\n",
        "# for i, estimator in enumerate(ovo_classifier.estimators_):\n",
        "#    print(f\"  Classifier for pair {i}: Coef - {estimator.coef_}, Intercept - {estimator.intercept_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBksfQj8frDn",
        "outputId": "a56ffebc-cfeb-42eb-bed1-c14d760af89a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with One-vs-One (OvO): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "# classification\n",
        "# Import necessry libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer # Using a binary classification dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "# We use the Breast Cancer dataset which is a binary classification problem\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data        # Features\n",
        "y = breast_cancer.target      # Target labels (0 or 1)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 7: Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=breast_cancer.target_names,\n",
        "            yticklabels=breast_cancer.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Explanation of the confusion matrix (based on a 2x2 matrix for binary classification):\n",
        "# The rows represent the actual classes, and the columns represent the predicted classes.\n",
        "# Top-left (True Negatives - TN): Correctly predicted negative cases.\n",
        "# Top-right (False Positives - FP): Incorrectly predicted positive cases (Type I error).\n",
        "# Bottom-left (False Negatives - FN): Incorrectly predicted negative cases (Type II error).\n",
        "# Bottom-right (True Positives - TP): Correctly predicted positive cases.\n",
        "#\n",
        "# For the Breast Cancer dataset:\n",
        "# TN: Correctly predicted as benign (class 0).\n",
        "# FP: Incorrectly predicted as malignant (class 1) when it was benign.\n",
        "# FN: Incorrectly predicted as benign (class 0) when it was malignant.\n",
        "# TP: Correctly predicted as malignant (class 1).\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "KT_f8cztf-Co",
        "outputId": "4e646739-3dbd-4137-fbdf-d574cce23a31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "# Recall, and F1-Score\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer # Using a binary classification dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "# We use the Breast Cancer dataset which is a binary classification problem\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data        # Features\n",
        "y = breast_cancer.target      # Target labels (0 or 1)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance using different metrics\n",
        "\n",
        "# Accuracy: Proportion of correct predictions among the total number of cases examined.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Precision: The ratio TP / (TP + FP). It is the ability of the classifier not to label as positive a negative sample.\n",
        "# For binary classification, precision_score defaults to 'binary' for the positive label.\n",
        "# If you have a multiclass problem, you would need to specify 'average' parameter.\n",
        "precision = precision_score(y_test, y_pred)\n",
        "\n",
        "# Recall: The ratio TP / (TP + FN). It is the ability of the classifier to find all the positive samples.\n",
        "# Also known as Sensitivity or True Positive Rate.\n",
        "# For binary classification, recall_score defaults to 'binary' for the positive label.\n",
        "# If you have a multiclass problem, you would need to specify 'average' parameter.\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# F1-Score: The harmonic mean of precision and recall. A weighted average of precision and recall.\n",
        "# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "# It ranges from 0 to 1, where 1 is the best F1 score.\n",
        "# For binary classification, f1_score defaults to 'binary' for the positive label.\n",
        "# If you have a multiclass problem, you would need to specify 'average' parameter.\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Note on multiclass classification for Precision, Recall, F1-Score:\n",
        "# For multiclass problems, these metrics need an 'average' parameter (e.g., 'micro', 'macro', 'weighted').\n",
        "# 'micro': Calculate metrics globally by counting total true positives, false negatives, and false positives.\n",
        "# 'macro': Calculate metrics for each label, and find their unweighted mean. Doesn't account for label imbalance.\n",
        "# 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).\n",
        "# For the Iris dataset (3 classes), you would typically use:\n",
        "# precision = precision_score(y_test, y_pred, average='weighted')\n",
        "# recall = recall_score(y_test, y_pred, average='weighted')\n",
        "# f1 = f1_score(y_test, y_pred, average='weighted')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlY1W_S9gPIr",
        "outputId": "a4e2f876-2915-47f5-8107-87db1f4c61eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Metrics:\n",
            "Accuracy:  0.9561\n",
            "Precision: 0.9459\n",
            "Recall:    0.9859\n",
            "F1-Score:  0.9655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "# improve model performance\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Create an imbalanced synthetic dataset\n",
        "# make_classification generates a random n-class classification problem.\n",
        "# weights parameter allows specifying the proportion of samples per class.\n",
        "# n_samples: total number of samples\n",
        "# n_features: total number of features\n",
        "# n_informative: number of informative features\n",
        "# n_redundant: number of redundant features\n",
        "# n_clusters_per_class: number of clusters per class\n",
        "# weights: list or tuple specifying the proportion of samples assigned to each class.\n",
        "#    e.g., [0.99, 0.01] means 99% class 0, 1% class 1 (imbalanced binary)\n",
        "# flip_y: fraction of samples whose class is assigned randomly.\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5,\n",
        "                           n_clusters_per_class=2, weights=[0.95, 0.05], flip_y=0.01, random_state=42)\n",
        "\n",
        "# Print class distribution\n",
        "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
        "print(\"Original Dataset Class Distribution:\")\n",
        "for cls, count in zip(unique_classes, class_counts):\n",
        "    print(f\"Class {cls}: {count} samples ({count/len(y):.2f}%)\")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "# Stratify the split to maintain the class distribution in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Print train/test class distribution (should be similar to original)\n",
        "print(\"\\nTrain Set Class Distribution:\")\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "for cls, count in zip(unique_train, counts_train):\n",
        "    print(f\"Class {cls}: {count} samples ({count/len(y_train):.2f}%)\")\n",
        "\n",
        "print(\"\\nTest Set Class Distribution:\")\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "for cls, count in zip(unique_test, counts_test):\n",
        "    print(f\"Class {cls}: {count} samples ({count/len(y_test):.2f}%)\")\n",
        "\n",
        "\n",
        "# Step 3: Train Logistic Regression WITHOUT class weights\n",
        "print(\"\\n--- Training Logistic Regression WITHOUT Class Weights ---\")\n",
        "model_no_weights = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model without weights\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "accuracy_no_weights = accuracy_score(y_test, y_pred_no_weights)\n",
        "report_no_weights = classification_report(y_test, y_pred_no_weights)\n",
        "cm_no_weights = confusion_matrix(y_test, y_pred_no_weights)\n",
        "\n",
        "print(f\"Accuracy (No Weights): {accuracy_no_weights:.4f}\")\n",
        "print(\"\\nClassification Report (No Weights):\\n\", report_no_weights)\n",
        "print(\"\\nConfusion Matrix (No Weights):\\n\", cm_no_weights)\n",
        "\n",
        "# Visualize Confusion Matrix (No Weights)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_no_weights, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=unique_classes, yticklabels=unique_classes)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix (No Class Weights)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Step 4: Train Logistic Regression WITH class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "jlYNAUiYhSMI",
        "outputId": "1d15300f-9b47-41e1-d44e-87547c2f6b41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset Class Distribution:\n",
            "Class 0: 942 samples (0.94%)\n",
            "Class 1: 58 samples (0.06%)\n",
            "\n",
            "Train Set Class Distribution:\n",
            "Class 0: 754 samples (0.94%)\n",
            "Class 1: 46 samples (0.06%)\n",
            "\n",
            "Test Set Class Distribution:\n",
            "Class 0: 188 samples (0.94%)\n",
            "Class 1: 12 samples (0.06%)\n",
            "\n",
            "--- Training Logistic Regression WITHOUT Class Weights ---\n",
            "Accuracy (No Weights): 0.9350\n",
            "\n",
            "Classification Report (No Weights):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.97       188\n",
            "           1       0.33      0.08      0.13        12\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.64      0.54      0.55       200\n",
            "weighted avg       0.91      0.94      0.92       200\n",
            "\n",
            "\n",
            "Confusion Matrix (No Weights):\n",
            " [[186   2]\n",
            " [ 11   1]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "# evaluate performance\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "# You can download the dataset from Kaggle or other sources.\n",
        "# Make sure the 'titanic.csv' file is in the correct directory, or provide the full path.\n",
        "try:\n",
        "    # **Ensure 'titanic.csv' is in the correct directory or provide the full path**\n",
        "    df = pd.read_csv('titanic.csv')\n",
        "    print(\"Titanic dataset loaded successfully.\")\n",
        "\n",
        "    # Step 2: Initial Data Inspection\n",
        "    print(\"\\n--- Initial Data Inspection ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    df.info()\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nValue counts for 'Survived':\")\n",
        "    print(df['Survived'].value_counts())\n",
        "\n",
        "\n",
        "    # Step 3: Feature Selection and Preprocessing\n",
        "\n",
        "    # Identify target variable and features\n",
        "    X = df.drop('Survived', axis=1)\n",
        "    y = df['Survived']\n",
        "\n",
        "    # Select relevant features for the model\n",
        "    # We'll select some features and handle their specific data types and missing values.\n",
        "    # 'PassengerId', 'Name', 'Ticket' are usually not directly used as features\n",
        "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "    X = X[features]\n",
        "\n",
        "    print(\"\\nSelected Features:\")\n",
        "    print(X.head())\n",
        "\n",
        "    # Define preprocessing steps for different types of features\n",
        "\n",
        "    # Numerical features: 'Age', 'Fare', 'SibSp', 'Parch'\n",
        "    # Handle missing values using median imputation.\n",
        "    # Note: SibSp and Parch have no missing values in the standard dataset,\n",
        "    # but it's good practice to include them in the numerical pipeline if imputation is needed.\n",
        "    numerical_features = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')) # Use median for numerical data\n",
        "    ])\n",
        "\n",
        "    # Categorical features: 'Pclass', 'Sex', 'Embarked'\n",
        "    # Handle missing values using most frequent value imputation.\n",
        "    # Use One-Hot Encoding for categorical features.\n",
        "    categorical_features = ['Pclass', 'Sex', 'Embarked']\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')), # Use most frequent for categorical data\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))    # Handle unknown categories during prediction\n",
        "    ])\n",
        "\n",
        "    # Create a column transformer to apply different transformations to different columns\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Step 4: Create the full pipeline\n",
        "    # Combine the preprocessor and the Logistic Regression model into a pipeline\n",
        "    model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', model)])\n",
        "\n",
        "\n",
        "    # Step 5: Split the dataset into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "    print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "\n",
        "    # Step 6: Train the Logistic Regression model\n",
        "    print(\"\\n--- Training the model ---\")\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # Step 7: Predict on the test data\n",
        "    print(\"\\n--- Evaluating the model ---\")\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Step 8: Evaluate the model performance\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(class_report)\n",
        "\n",
        "    # Visualize the confusion matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Not Survived (0)', 'Survived (1)'],\n",
        "                yticklabels=['Not Survived (0)', 'Survived (1)'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix (Titanic Survival)')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n--- Evaluation Metrics Explanation ---\")\n",
        "    print(\"Accuracy: Overall proportion of correct predictions.\")\n",
        "    print(\"Precision (for class 1, Survived): Proportion of correctly predicted survivors out of all predicted survivors.\")\n",
        "    print(\"Recall (for class 1, Survived): Proportion of correctly predicted survivors out of all actual survivors (also known as Sensitivity).\")\n",
        "    print(\"F1-Score (for class 1, Survived): Harmonic mean of Precision and Recall, balancing both metrics.\")\n",
        "    print(\"Support: The number of actual occurrences of each class in the test set.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'titanic.csv' not found.\")\n",
        "    print(\"Please download the Titanic dataset and place it in the same directory as the script, or provide the full path.\")\n",
        "    # You might want to add a flag or raise a more specific error here\n",
        "    # instead of just printing and potentially continuing execution with missing data.\n",
        "    # For this example, we'll let the error message be the indication.\n",
        "except Exception as e:\n",
        "    # Catch any other exceptions during data loading or processing\n",
        "    print(f\"An error occurred during data loading or processing: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCCd2GFShu2q",
        "outputId": "8d53413e-55f7-4498-e115-ac03a950d7c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'titanic.csv' not found.\n",
            "Please download the Titanic dataset and place it in the same directory as the script, or provide the full path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "# model. Evaluate its accuracy and compare results with and without scaling\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "from sklearn.pipeline import Pipeline         # Import Pipeline\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model WITHOUT scaling\n",
        "print(\"--- Training Logistic Regression WITHOUT Scaling ---\")\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate without scaling\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Model Accuracy (No Scaling): {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# Step 4: Train Logistic Regression model WITH scaling\n",
        "print(\"\\n--- Training Logistic Regression WITH Scaling ---\")\n",
        "\n",
        "# Create a Pipeline that first scales the data and then trains the model\n",
        "# Standardization scales features to have a mean of 0 and a standard deviation of 1.\n",
        "# This is important for algorithms sensitive to feature magnitudes, like Logistic Regression\n",
        "# with regularization.\n",
        "# Source [1] highlights how standardization can improve model accuracy.\n",
        "pipeline_with_scaling = Pipeline([\n",
        "    ('scaler', StandardScaler()),             # Step 1: Apply Standardization\n",
        "    ('logistic_regression', LogisticRegression(max_iter=200)) # Step 2: Train Logistic Regression\n",
        "])\n",
        "\n",
        "# Train the pipeline (it will first scale the data and then fit the model)\n",
        "pipeline_with_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the pipeline (it will first scale the test data and then predict)\n",
        "y_pred_with_scaling = pipeline_with_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(f\"Model Accuracy (With Scaling): {accuracy_with_scaling:.4f}\")\n",
        "\n",
        "# Step 5: Compare results\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Accuracy Difference (With Scaling - No Scaling): {accuracy_with_scaling - accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# Note: For the Iris dataset, which has features with relatively similar scales,\n",
        "# the improvement from scaling might be minimal or even zero.\n",
        "# Scaling is generally more impactful when features have vastly different ranges\n",
        "# (e.g., one feature from 0-1000 and another from 0-1)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVx2F4aXi4kZ",
        "outputId": "7b4d4cec-4cde-4bc3-d4b8-28216a15a855"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Logistic Regression WITHOUT Scaling ---\n",
            "Model Accuracy (No Scaling): 1.0000\n",
            "\n",
            "--- Training Logistic Regression WITH Scaling ---\n",
            "Model Accuracy (With Scaling): 1.0000\n",
            "\n",
            "--- Comparison ---\n",
            "Accuracy Difference (With Scaling - No Scaling): 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer # Using a binary classification dataset suitable for ROC-AUC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score # Import roc_auc_score and roc_curve\n",
        "import matplotlib.pyplot as plt # For plotting the ROC curve\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "# ROC-AUC is typically used for binary classification problems.\n",
        "# The Breast Cancer dataset is a good example with two classes (malignant/benign).\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data        # Features\n",
        "y = breast_cancer.target      # Target labels (0 or 1)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "# Use a solver that supports probability prediction (most do).\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities on test data\n",
        "# ROC-AUC requires predicted probabilities, not just the final class prediction.\n",
        "# model.predict_proba(X_test) returns a 2D array where each row corresponds to a sample\n",
        "# and columns represent the probability of that sample belonging to each class.\n",
        "# For binary classification, the columns are usually [probability of class 0, probability of class 1].\n",
        "# We need the probability of the positive class (class 1).\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Optional: Also get the hard predictions for standard accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Step 5: Calculate the ROC-AUC score\n",
        "# roc_auc_score calculates the Area Under the Receiver Operating Characteristic Curve.\n",
        "# A higher AUC indicates better model performance at distinguishing between positive and negative classes.\n",
        "# AUC ranges from 0 to 1. An AUC of 0.5 suggests random guessing, while an AUC of 1 is a perfect classifier.\n",
        "# Source [1] mentions AUC as the area under the ROC curve.\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Step 6: Print the ROC-AUC score\n",
        "print(f\"\\nModel ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 7: Generate and Plot the ROC Curve (Optional but Recommended)\n",
        "# The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate\n",
        "# at various threshold settings.\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing') # Diagonal line for random classifier\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- ROC-AUC Score Explanation ---\")\n",
        "print(\"ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) is a performance metric\")\n",
        "print(\"for binary classification problems. It measures the ability of a classifier to distinguish\")\n",
        "print(\"between positive and negative classes across different probability thresholds.\")\n",
        "print(\"An AUC score close to 1.0 indicates a strong classifier, while a score close to 0.5\")\n",
        "print(\"suggests performance no better than random guessing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "3zejtXxojPAg",
        "outputId": "3988317f-2891-4e1f-dbd0-356e107bb428"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9561\n",
            "\n",
            "Model ROC-AUC Score: 0.9977\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ROC-AUC Score Explanation ---\n",
            "ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) is a performance metric\n",
            "for binary classification problems. It measures the ability of a classifier to distinguish\n",
            "between positive and negative classes across different probability thresholds.\n",
            "An AUC score close to 1.0 indicates a strong classifier, while a score close to 0.5\n",
            "suggests performance no better than random guessing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "# accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model with a custom C value\n",
        "# In scikit-learn's LogisticRegression, the parameter 'C' is the inverse of\n",
        "# regularization strength. Smaller values of C specify stronger regularization.\n",
        "# A larger C (like C=0.5 compared to a default often around 1) means less regularization.\n",
        "# Note: There isn't a direct \"learning rate\" parameter like in gradient descent\n",
        "# implementations. The 'solver' parameter controls the optimization algorithm.\n",
        "# 'C' is related to the penalty term.\n",
        "custom_c_value = 0.5\n",
        "\n",
        "print(f\"--- Training Logistic Regression with C = {custom_c_value} ---\")\n",
        "model = LogisticRegression(C=custom_c_value, max_iter=200) # Set the custom C value\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the accuracy\n",
        "print(f\"Model Accuracy with C = {custom_c_value}: {accuracy:.4f}\")\n",
        "\n",
        "# Note on C:\n",
        "# C is the inverse of regularization strength.\n",
        "# High C: Less regularization (model fits training data more closely, potentially overfitting)\n",
        "# Low C: More regularization (model is simpler, potentially underfitting)\n",
        "# The \"best\" C value is typically found through hyperparameter tuning (like GridSearchCV or RandomizedSearchCV, as shown in previous examples)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_PKHsCcjl4x",
        "outputId": "70c1ba38-fd21-4270-85bd-0de0463c4052"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Logistic Regression with C = 0.5 ---\n",
            "Model Accuracy with C = 0.5: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "# coefficients\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd # Often useful for handling feature names\n",
        "from sklearn.datasets import load_iris # Using a dataset with named features\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# We'll use the Iris dataset as it has readily available feature names.\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features (numpy array)\n",
        "y = iris.target      # Target labels\n",
        "feature_names = iris.feature_names # List of feature names\n",
        "\n",
        "# Optional: Convert X to a pandas DataFrame to easily associate coefficients with names\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "# Use X_df if you converted to DataFrame, otherwise use X\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "# For multi-class problems like Iris, Logistic Regression trains one classifier per class\n",
        "# (or one per pair of classes with OvO). The coefficients will be a matrix (n_classes, n_features).\n",
        "# Let's use the default 'auto' multi_class setting, which typically uses 'ovr' for this case.\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear') # liblinear works well for smaller datasets and OvR\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate (optional, but good practice)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 5: Identify important features based on coefficients\n",
        "# The absolute magnitude of the coefficient for a feature indicates its importance\n",
        "# in predicting the target variable. A larger absolute value means the feature has a\n",
        "# stronger influence on the outcome. The sign of the coefficient indicates the\n",
        "# direction of the relationship (positive or negative).\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "\n",
        "# For multi-class problems (like Iris), model.coef_ is a 2D array (n_classes, n_features).\n",
        "# Each row corresponds to the coefficients for predicting one class against the rest (OvR).\n",
        "# We can look at the coefficients for each class or find the overall importance\n",
        "# (e.g., sum of absolute values across classes).\n",
        "\n",
        "# Let's look at coefficients for each class (Iris has 3 classes: 0, 1, 2)\n",
        "# The target names are: ['setosa', 'versicolor', 'virginica']\n",
        "target_names = iris.target_names\n",
        "\n",
        "for i, class_name in enumerate(target_names):\n",
        "    print(f\"\\n--- Coefficients for Class '{class_name}' (predicting this class vs others) ---\")\n",
        "    # Get coefficients for the current class\n",
        "    coef_for_class = model.coef_[i]\n",
        "\n",
        "    # Pair feature names with coefficients\n",
        "    feature_coef_pairs = list(zip(feature_names, coef_for_class))\n",
        "\n",
        "    # Sort by absolute coefficient value to see most important features for this class\n",
        "    sorted_feature_coef_pairs = sorted(feature_coef_pairs, key=lambda item: abs(item[1]), reverse=True)\n",
        "\n",
        "    # Print sorted coefficients\n",
        "    for feature, coef in sorted_feature_coef_pairs:\n",
        "        print(f\"  {feature}: {coef:.4f}\")\n",
        "\n",
        "# To get a single measure of \"overall\" feature importance across all classes (OvR):\n",
        "# We can take the sum of the absolute values of the coefficients for each feature across all classes.\n",
        "overall_importance = np.sum(np.abs(model.coef_), axis=0)\n",
        "\n",
        "print(\"\\n--- Overall Feature Importance (Sum of Absolute Coefficients across classes) ---\")\n",
        "feature_importance_pairs = list(zip(feature_names, overall_importance))\n",
        "\n",
        "# Sort by overall importance\n",
        "sorted_overall_importance = sorted(feature_importance_pairs, key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Print sorted overall importance\n",
        "for feature, importance in sorted_overall_importance:\n",
        "    print(f\"  {feature}: {importance:.4f}\")\n",
        "\n",
        "# Step 6: Print model intercept (optional)\n",
        "print(\"\\nModel Intercepts (for each class):\")\n",
        "# For multi-class OvR, there's an intercept for each binary classifier\n",
        "print(model.intercept_)\n",
        "\n",
        "# Interpretation:\n",
        "# Features with larger absolute coefficients (either positive or negative) are considered more important\n",
        "# by the model for distinguishing between the classes. For example, a large positive coefficient for\n",
        "# 'petal width (cm)' when predicting 'virginica' means that larger petal widths strongly indicate the 'virginica' class."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VITuE-48j2zn",
        "outputId": "a26b311c-f512-4e4b-ccb7-c8e48b0b5283"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Model Coefficients:\n",
            "\n",
            "--- Coefficients for Class 'setosa' (predicting this class vs others) ---\n",
            "  petal length (cm): -2.1521\n",
            "  sepal width (cm): 1.4097\n",
            "  petal width (cm): -0.9547\n",
            "  sepal length (cm): 0.3711\n",
            "\n",
            "--- Coefficients for Class 'versicolor' (predicting this class vs others) ---\n",
            "  sepal width (cm): -1.5890\n",
            "  petal width (cm): -1.1119\n",
            "  sepal length (cm): 0.4940\n",
            "  petal length (cm): 0.4372\n",
            "\n",
            "--- Coefficients for Class 'virginica' (predicting this class vs others) ---\n",
            "  petal length (cm): 2.3987\n",
            "  petal width (cm): 2.1556\n",
            "  sepal width (cm): -1.5889\n",
            "  sepal length (cm): -1.5590\n",
            "\n",
            "--- Overall Feature Importance (Sum of Absolute Coefficients across classes) ---\n",
            "  petal length (cm): 4.9880\n",
            "  sepal width (cm): 4.5876\n",
            "  petal width (cm): 4.2222\n",
            "  sepal length (cm): 2.4241\n",
            "\n",
            "Model Intercepts (for each class):\n",
            "[ 0.2478905   0.86408083 -1.00411267]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "# Score\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris # Using a multi-class dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, cohen_kappa_score # Import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Cohen's Kappa can be used for both binary and multi-class classification.\n",
        "# We'll use the Iris dataset as an example of multi-class.\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance using Accuracy and Cohen's Kappa Score\n",
        "\n",
        "# Accuracy: Simple proportion of correct predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Cohen's Kappa Score: Measures the agreement between two raters (in this case, the model's\n",
        "# predictions and the true labels) beyond chance agreement. It's often more robust\n",
        "# than accuracy, especially on imbalanced datasets.\n",
        "# Formula: kappa = (p_o - p_e) / (1 - p_e)\n",
        "# p_o is the observed agreement (Accuracy).\n",
        "# p_e is the expected agreement by chance.\n",
        "# Kappa ranges typically from -1 to 1:\n",
        "# 1: Perfect agreement\n",
        "# 0: Agreement is the same as chance\n",
        "# < 0: Agreement is worse than chance\n",
        "# Cohen's Kappa is suitable for both binary and multi-class problems.\n",
        "# Source [1] discusses kappa coefficient as a measure of inter-rater reliability,\n",
        "# which can be applied to classifier agreement with ground truth.\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy:        {accuracy:.4f}\")\n",
        "print(f\"Cohen's Kappa:   {kappa_score:.4f}\")\n",
        "\n",
        "print(\"\\n--- Cohen's Kappa Explanation ---\")\n",
        "print(\"Cohen's Kappa measures the agreement between the model's predictions and the true labels,\")\n",
        "print(\"accounting for the possibility of agreement occurring by chance.\")\n",
        "print(\"Interpretation guidelines (often cited):\")\n",
        "print(\"  < 0: Less than chance agreement\")\n",
        "print(\"  0.01 - 0.20: Slight agreement\")\n",
        "print(\"  0.21 - 0.40: Fair agreement\")\n",
        "print(\"  0.41 - 0.60: Moderate agreement\")\n",
        "print(\"  0.61 - 0.80: Substantial agreement\")\n",
        "print(\"  0.81 - 0.99: Almost perfect agreement\")\n",
        "print(\"  1.00: Perfect agreement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJR2SdNHkMcU",
        "outputId": "8c5528a0-fdd1-4d9e-d6b9-35c33ce1814f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Metrics:\n",
            "Accuracy:        1.0000\n",
            "Cohen's Kappa:   1.0000\n",
            "\n",
            "--- Cohen's Kappa Explanation ---\n",
            "Cohen's Kappa measures the agreement between the model's predictions and the true labels,\n",
            "accounting for the possibility of agreement occurring by chance.\n",
            "Interpretation guidelines (often cited):\n",
            "  < 0: Less than chance agreement\n",
            "  0.01 - 0.20: Slight agreement\n",
            "  0.21 - 0.40: Fair agreement\n",
            "  0.41 - 0.60: Moderate agreement\n",
            "  0.61 - 0.80: Substantial agreement\n",
            "  0.81 - 0.99: Almost perfect agreement\n",
            "  1.00: Perfect agreement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "# classificatio:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer # Using a binary classification dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score # Import necessary metrics\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "# The Precision-Recall Curve is particularly useful for imbalanced binary classification\n",
        "# problems where the positive class is rare. The Breast Cancer dataset is suitable here.\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data        # Features\n",
        "y = breast_cancer.target      # Target labels (0 or 1) - Binary problem\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "# Use a solver that supports probability prediction.\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities of the positive class on test data\n",
        "# The Precision-Recall Curve requires predicted probabilities, specifically the probability\n",
        "# of the positive class (class 1 in this dataset).\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, and Thresholds\n",
        "# precision_recall_curve computes precision-recall pairs for different probability thresholds.\n",
        "# The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold.\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Step 6: Calculate the Average Precision Score (AP)\n",
        "# The Average Precision score summarizes the Precision-Recall curve as the weighted mean\n",
        "# of precisions achieved at each threshold, with the increase in recall from the previous\n",
        "# threshold used as the weight. It is a single number summary of the PR curve.\n",
        "# Source [1] refers to AUC-PR as the area under the Precision-Recall curve,\n",
        "# and Average Precision (AP) is a common way to calculate this area.\n",
        "average_precision = average_precision_score(y_test, y_prob)\n",
        "\n",
        "# Step 7: Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.', label=f'Logistic Regression (AP = {average_precision:.2f})')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.show()\n",
        "\n",
        "print(f\"Model Average Precision (AP) Score: {average_precision:.4f}\")\n",
        "\n",
        "print(\"\\n--- Precision-Recall Curve Explanation ---\")\n",
        "print(\"The Precision-Recall curve plots Precision vs. Recall for different probability thresholds.\")\n",
        "print(\"Precision: Ability of the classifier not to label a negative sample as positive (TP / (TP + FP)).\")\n",
        "print(\"Recall: Ability of the classifier to find all the positive samples (TP / (TP + FN)).\")\n",
        "print(\"A good classifier has a curve that stays close to the top-right corner.\")\n",
        "print(\"The Average Precision (AP) score provides a single number summary of the curve.\")\n",
        "print(\"It is particularly informative for imbalanced datasets, where high accuracy might be\")\n",
        "print(\"misleading if the model simply predicts the majority class.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "Nwfj9vBgk1Q7",
        "outputId": "ba4eba4d-3d99-4be1-a650-1541c1d2a84f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Average Precision (AP) Score: 0.9987\n",
            "\n",
            "--- Precision-Recall Curve Explanation ---\n",
            "The Precision-Recall curve plots Precision vs. Recall for different probability thresholds.\n",
            "Precision: Ability of the classifier not to label a negative sample as positive (TP / (TP + FP)).\n",
            "Recall: Ability of the classifier to find all the positive samples (TP / (TP + FN)).\n",
            "A good classifier has a curve that stays close to the top-right corner.\n",
            "The Average Precision (AP) score provides a single number summary of the curve.\n",
            "It is particularly informative for imbalanced datasets, where high accuracy might be\n",
            "misleading if the model simply predicts the majority class.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "# their accuracy\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris # Using a dataset suitable for different solvers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define a dictionary of solvers to compare\n",
        "# Note: Different solvers have different strengths and are compatible with different penalties.\n",
        "# 'liblinear' works well for small datasets and supports 'l1' and 'l2' penalties.\n",
        "# 'lbfgs' is the default, works well for larger datasets, and supports 'l2' and 'none' penalties (and 'elasticnet' when multi_class='multinomial').\n",
        "# 'saga' is a stochastic average gradient descent solver, suitable for large datasets,\n",
        "# and supports 'l1', 'l2', 'elasticnet', and 'none' penalties. It can be faster than 'lbfgs' for very large datasets.\n",
        "solvers_to_compare = {\n",
        "    'liblinear': LogisticRegression(solver='liblinear', max_iter=200),\n",
        "    'lbfgs': LogisticRegression(solver='lbfgs', max_iter=200), # lbfgs is default\n",
        "    'saga': LogisticRegression(solver='saga', max_iter=1000) # saga might need more iterations\n",
        "}\n",
        "\n",
        "# For multi-class problems (like Iris), 'lbfgs' and 'saga' support 'auto', 'ovr', and 'multinomial'\n",
        "# multi_class options. 'liblinear' only supports 'ovr'. Since we're comparing solvers,\n",
        "# let's ensure compatibility. 'lbfgs' and 'saga' default to 'auto' which often uses 'multinomial'\n",
        "# for multi-class data. 'liblinear' uses 'ovr'. For a fair comparison across these,\n",
        "# we might force 'ovr' for all or handle solver/multi_class compatibility carefully.\n",
        "# For simplicity here, we'll use default multi_class ('auto' for lbfgs/saga, 'ovr' for liblinear)\n",
        "# as 'auto' chooses the appropriate strategy for multi-class data.\n",
        "# If using 'elasticnet' or 'none' penalties, only 'saga' or 'lbfgs' (with multi_class='multinomial')\n",
        "# would be options, respectively. The choice of penalty can restrict solver options.\n",
        "\n",
        "# Step 4: Train and evaluate models for each solver\n",
        "results = {}\n",
        "\n",
        "print(\"--- Comparing Logistic Regression Solvers ---\")\n",
        "\n",
        "for solver_name, model in solvers_to_compare.items():\n",
        "    print(f\"\\nTraining with solver: {solver_name}\")\n",
        "    try:\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results[solver_name] = accuracy\n",
        "\n",
        "        print(f\"Accuracy ({solver_name}): {accuracy:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch potential errors, e.g., convergence warnings or incompatibility issues\n",
        "        print(f\"Error training with {solver_name}: {e}\")\n",
        "        results[solver_name] = None # Store None if training failed\n",
        "\n",
        "\n",
        "# Step 5: Print comparison of results\n",
        "print(\"\\n--- Solver Comparison Results ---\")\n",
        "for solver_name, accuracy in results.items():\n",
        "    if accuracy is not None:\n",
        "        print(f\"{solver_name}: {accuracy:.4f}\")\n",
        "    else:\n",
        "        print(f\"{solver_name}: Training Failed\")\n",
        "\n",
        "# Note on Solver Choice:\n",
        "# The best solver depends on the dataset size, the penalty ('l1', 'l2', 'elasticnet', 'none'),\n",
        "# and whether it's binary or multi-class.\n",
        "# - 'liblinear': Good for small datasets, only supports 'l1' and 'l2', and 'ovr' multi_class.\n",
        "# - 'lbfgs': Default, generally good for moderate to large datasets, supports 'l2' and 'none' (and 'elasticnet' for multinomial).\n",
        "# - 'saga': Good for large datasets, supports all penalties, and can be faster with many samples.\n",
        "# - 'newton-cg', 'sag', 'svd': Other options with different characteristics and compatibilities.\n",
        "# Always check the scikit-learn documentation for the latest details on solver compatibility and recommendations."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nNr6IWMlZ7s",
        "outputId": "8a233247-fac6-480e-e34b-33e9662e1f08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Comparing Logistic Regression Solvers ---\n",
            "\n",
            "Training with solver: liblinear\n",
            "Accuracy (liblinear): 1.0000\n",
            "\n",
            "Training with solver: lbfgs\n",
            "Accuracy (lbfgs): 1.0000\n",
            "\n",
            "Training with solver: saga\n",
            "Accuracy (saga): 1.0000\n",
            "\n",
            "--- Solver Comparison Results ---\n",
            "liblinear: 1.0000\n",
            "lbfgs: 1.0000\n",
            "saga: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22.M Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "# Correlation Coefficient (MCC)\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer # Using a binary classification dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef # Import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load a binary classification dataset\n",
        "# The Matthews Correlation Coefficient (MCC) is a single metric that summarizes the performance\n",
        "# of a classifier, particularly useful for imbalanced binary problems.\n",
        "# We'll use the Breast Cancer dataset.\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data        # Features\n",
        "y = breast_cancer.target      # Target labels (0 or 1) - Binary problem\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000) # Increased max_iter for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance using Accuracy and Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "# Accuracy: Simple proportion of correct predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Matthews Correlation Coefficient (MCC): A measure of the quality of binary (two-class)\n",
        "# classifications. It takes into account true and false positives and negatives and\n",
        "# is generally regarded as a balanced measure which can be used even if the classes\n",
        "# are of very different sizes.\n",
        "# MCC is in essence a correlation coefficient between the observed and predicted\n",
        "# binary classifications.\n",
        "# MCC values range from -1 to +1:\n",
        "# +1: Perfect prediction\n",
        "# 0: Average random prediction\n",
        "# -1: Inverse prediction (predicts opposite of the truth)\n",
        "# Source [1] confirms MCC range and interpretation.\n",
        "# Note: matthews_corrcoef is primarily defined for binary classification, though\n",
        "# extensions for multiclass exist but are less commonly used or may require different\n",
        "# interpretations.\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(\"Model Evaluation Metrics:\")\n",
        "print(f\"Accuracy:                         {accuracy:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc_score:.4f}\")\n",
        "\n",
        "print(\"\\n--- Matthews Correlation Coefficient (MCC) Explanation ---\")\n",
        "print(\"MCC is a balanced measure of classification performance that accounts for all four\")\n",
        "print(\"cells of the confusion matrix (True Positives, True Negatives, False Positives, False Negatives).\")\n",
        "print(\"It provides a more reliable score than accuracy or F1 score when dealing with imbalanced datasets.\")\n",
        "print(\"A score of +1 represents a perfect prediction, 0 a random prediction, and -1 an inverse prediction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBBTw7Y6l3aN",
        "outputId": "3787ee6e-71b1-4a63-8e24-07f0908e3a73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Metrics:\n",
            "Accuracy:                         0.9561\n",
            "Matthews Correlation Coefficient: 0.9068\n",
            "\n",
            "--- Matthews Correlation Coefficient (MCC) Explanation ---\n",
            "MCC is a balanced measure of classification performance that accounts for all four\n",
            "cells of the confusion matrix (True Positives, True Negatives, False Positives, False Negatives).\n",
            "It provides a more reliable score than accuracy or F1 score when dealing with imbalanced datasets.\n",
            "A score of +1 represents a perfect prediction, 0 a random prediction, and -1 an inverse prediction.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "# accuracy to see the impact of feature scaling\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris # Using a dataset where scaling might have an impact\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler # Import StandardScaler\n",
        "from sklearn.pipeline import Pipeline         # Import Pipeline\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# We'll use the Iris dataset again. Although the feature scales are somewhat similar,\n",
        "# we can still demonstrate the effect of standardization. Datasets with features\n",
        "# on vastly different scales (e.g., one feature in thousands, another in single digits)\n",
        "# show a more pronounced impact of scaling.\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"--- Comparing Logistic Regression on Raw vs. Standardized Data ---\")\n",
        "\n",
        "# --- Train on Raw Data ---\n",
        "print(\"\\n--- Training Logistic Regression on RAW Data ---\")\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on raw data\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(f\"Model Accuracy (Raw Data): {accuracy_raw:.4f}\")\n",
        "\n",
        "\n",
        "# --- Train on Standardized Data ---\n",
        "print(\"\\n--- Training Logistic Regression on STANDARDIZED Data ---\")\n",
        "\n",
        "# Create a Pipeline that first scales the data and then trains the model\n",
        "# Standardization is crucial for many algorithms that use distance metrics\n",
        "# or are affected by the magnitude of features, including Logistic Regression\n",
        "# especially when regularization is used (which is the default).\n",
        "# Source [1] mentions how scaling is important for methods sensitive to feature scales.\n",
        "pipeline_standardized = Pipeline([\n",
        "    ('scaler', StandardScaler()),             # Step 1: Apply Standardization\n",
        "    ('logistic_regression', LogisticRegression(max_iter=200)) # Step 2: Train Logistic Regression\n",
        "])\n",
        "\n",
        "# Train the pipeline (it will first scale the training data and then fit the model)\n",
        "pipeline_standardized.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the pipeline (it will first scale the test data and then predict)\n",
        "y_pred_standardized = pipeline_standardized.predict(X_test)\n",
        "\n",
        "# Evaluate on standardized data\n",
        "accuracy_standardized = accuracy_score(y_test, y_pred_standardized)\n",
        "print(f\"Model Accuracy (Standardized Data): {accuracy_standardized:.4f}\")\n",
        "\n",
        "\n",
        "# --- Compare Results ---\n",
        "print(\"\\n--- Comparison of Accuracy ---\")\n",
        "print(f\"Accuracy (Raw Data):          {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy (Standardized Data): {accuracy_standardized:.4f}\")\n",
        "\n",
        "# Highlight the difference\n",
        "if accuracy_standardized > accuracy_raw:\n",
        "    print(f\"Standardization improved accuracy by: {accuracy_standardized - accuracy_raw:.4f}\")\n",
        "elif accuracy_standardized < accuracy_raw:\n",
        "     print(f\"Standardization decreased accuracy by: {accuracy_raw - accuracy_standardized:.4f}\")\n",
        "else:\n",
        "     print(\"Standardization did not change accuracy.\")\n",
        "\n",
        "print(\"\\n--- Impact of Feature Scaling Explanation ---\")\n",
        "print(\"Feature scaling, like Standardization, is important for algorithms that are sensitive\")\n",
        "print(\"to the scale of input features. Logistic Regression, especially with default L2 regularization,\")\n",
        "print(\"can benefit from scaling because the regularization penalty is applied based on the magnitude\")\n",
        "print(\"of the coefficients, which are in turn influenced by the scale of the features.\")\n",
        "print(\"Without scaling, features with larger values might disproportionately affect the model.\")\n",
        "print(\"For datasets where features have vastly different ranges, scaling can significantly\")\n",
        "print(\"improve convergence speed and model performance.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpjnqKonmLKJ",
        "outputId": "df038fec-60a3-4f68-cd45-ed3fc043ee1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Comparing Logistic Regression on Raw vs. Standardized Data ---\n",
            "\n",
            "--- Training Logistic Regression on RAW Data ---\n",
            "Model Accuracy (Raw Data): 1.0000\n",
            "\n",
            "--- Training Logistic Regression on STANDARDIZED Data ---\n",
            "Model Accuracy (Standardized Data): 1.0000\n",
            "\n",
            "--- Comparison of Accuracy ---\n",
            "Accuracy (Raw Data):          1.0000\n",
            "Accuracy (Standardized Data): 1.0000\n",
            "Standardization did not change accuracy.\n",
            "\n",
            "--- Impact of Feature Scaling Explanation ---\n",
            "Feature scaling, like Standardization, is important for algorithms that are sensitive\n",
            "to the scale of input features. Logistic Regression, especially with default L2 regularization,\n",
            "can benefit from scaling because the regularization penalty is applied based on the magnitude\n",
            "of the coefficients, which are in turn influenced by the scale of the features.\n",
            "Without scaling, features with larger values might disproportionately affect the model.\n",
            "For datasets where features have vastly different ranges, scaling can significantly\n",
            "improve convergence speed and model performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "# cross-validation\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "# While we'll use cross-validation on the *training* data to find the best C,\n",
        "# it's still good practice to hold out a separate test set for final evaluation\n",
        "# of the model with the best found hyperparameters.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"--- Finding Optimal C for Logistic Regression using Cross-Validation ---\")\n",
        "\n",
        "# Step 3: Define a range of C values to test\n",
        "# 'C' is the inverse of regularization strength. Smaller C means stronger regularization.\n",
        "# We typically test values on a logarithmic scale.\n",
        "c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "\n",
        "# Step 4: Set up Cross-Validation\n",
        "# We'll use K-Fold cross-validation. 5 folds is a common choice.\n",
        "# KFold shuffles the data if shuffle=True. StratifiedKFold is preferred for classification\n",
        "# to ensure class proportions are maintained in each fold, especially with imbalanced data.\n",
        "# For simplicity with Iris (balanced), KFold is okay, but StratifiedKFold is safer.\n",
        "# Let's use StratifiedKFold.\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 5: Evaluate Logistic Regression for each C value using cross-validation\n",
        "cv_scores = []\n",
        "\n",
        "for c in c_values:\n",
        "    print(f\"\\nEvaluating with C = {c}\")\n",
        "    # Create a Logistic Regression model with the current C value\n",
        "    # Using a solver compatible with the default penalty ('l2' for lbfgs).\n",
        "    # For larger C values or more complex datasets, you might need to increase max_iter.\n",
        "    model = LogisticRegression(C=c, max_iter=200, solver='liblinear') # liblinear often converges faster\n",
        "\n",
        "    # Perform cross-validation\n",
        "    # cross_val_score returns an array of scores (e.g., accuracy for each fold)\n",
        "    # Source [1] highlights using cross_val_score for evaluating model performance across folds.\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_splitter, scoring='accuracy')\n",
        "\n",
        "    # Calculate the mean score across folds\n",
        "    mean_score = np.mean(scores)\n",
        "    cv_scores.append(mean_score)\n",
        "\n",
        "    print(f\"  Cross-validation accuracies: {scores}\")\n",
        "    print(f\"  Average cross-validation accuracy: {mean_score:.4f}\")\n",
        "\n",
        "# Step 6: Find the C value with the best average cross-validation score\n",
        "best_c_index = np.argmax(cv_scores)\n",
        "optimal_c = c_values[best_c_index]\n",
        "best_cv_accuracy = cv_scores[best_c_index]\n",
        "\n",
        "print(\"\\n--- Cross-Validation Results ---\")\n",
        "for c, score in zip(c_values, cv_scores):\n",
        "     print(f\"C = {c}: Average CV Accuracy = {score:.4f}\")\n",
        "\n",
        "print(f\"\\nOptimal C found via cross-validation: {optimal_c}\")\n",
        "print(f\"Best average cross-validation accuracy: {best_cv_accuracy:.4f}\")\n",
        "\n",
        "# Step 7: Train the final model on the entire training set using the optimal C\n",
        "print(f\"\\n--- Training final model with optimal C ({optimal_c}) on the full training set ---\")\n",
        "final_model = LogisticRegression(C=optimal_c, max_iter=200, solver='liblinear')\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the final model on the held-out test set\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"\\nFinal Model Accuracy on Test Set (using optimal C): {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n--- Explanation ---\")\n",
        "print(\"We used cross-validation on the training data to evaluate how different values\")\n",
        "print(\"of the regularization parameter C affect model performance (accuracy).\")\n",
        "print(\"The C value that resulted in the highest average cross-validation accuracy\")\n",
        "print(\"is considered the optimal C.\")\n",
        "print(\"Finally, the model was retrained using this optimal C value on the entire\")\n",
        "print(\"training dataset and evaluated on a separate, held-out test set to get\")\n",
        "print(\"a final, unbiased estimate of its performance.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rabAqyd0mhgT",
        "outputId": "4309765d-acd3-4a00-b8f5-c029d45be7f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Finding Optimal C for Logistic Regression using Cross-Validation ---\n",
            "\n",
            "Evaluating with C = 0.001\n",
            "  Cross-validation accuracies: [0.29166667 0.33333333 0.375      0.41666667 0.375     ]\n",
            "  Average cross-validation accuracy: 0.3583\n",
            "\n",
            "Evaluating with C = 0.01\n",
            "  Cross-validation accuracies: [0.625      0.66666667 0.66666667 0.66666667 0.66666667]\n",
            "  Average cross-validation accuracy: 0.6583\n",
            "\n",
            "Evaluating with C = 0.1\n",
            "  Cross-validation accuracies: [0.75       0.83333333 0.91666667 0.875      0.83333333]\n",
            "  Average cross-validation accuracy: 0.8417\n",
            "\n",
            "Evaluating with C = 1\n",
            "  Cross-validation accuracies: [0.95833333 0.95833333 1.         1.         0.91666667]\n",
            "  Average cross-validation accuracy: 0.9667\n",
            "\n",
            "Evaluating with C = 10\n",
            "  Cross-validation accuracies: [0.95833333 0.95833333 0.95833333 1.         0.95833333]\n",
            "  Average cross-validation accuracy: 0.9667\n",
            "\n",
            "Evaluating with C = 100\n",
            "  Cross-validation accuracies: [1.         0.95833333 0.91666667 1.         0.91666667]\n",
            "  Average cross-validation accuracy: 0.9583\n",
            "\n",
            "Evaluating with C = 1000\n",
            "  Cross-validation accuracies: [1.         0.95833333 0.91666667 1.         0.875     ]\n",
            "  Average cross-validation accuracy: 0.9500\n",
            "\n",
            "--- Cross-Validation Results ---\n",
            "C = 0.001: Average CV Accuracy = 0.3583\n",
            "C = 0.01: Average CV Accuracy = 0.6583\n",
            "C = 0.1: Average CV Accuracy = 0.8417\n",
            "C = 1: Average CV Accuracy = 0.9667\n",
            "C = 10: Average CV Accuracy = 0.9667\n",
            "C = 100: Average CV Accuracy = 0.9583\n",
            "C = 1000: Average CV Accuracy = 0.9500\n",
            "\n",
            "Optimal C found via cross-validation: 1\n",
            "Best average cross-validation accuracy: 0.9667\n",
            "\n",
            "--- Training final model with optimal C (1) on the full training set ---\n",
            "\n",
            "Final Model Accuracy on Test Set (using optimal C): 1.0000\n",
            "\n",
            "--- Explanation ---\n",
            "We used cross-validation on the training data to evaluate how different values\n",
            "of the regularization parameter C affect model performance (accuracy).\n",
            "The C value that resulted in the highest average cross-validation accuracy\n",
            "is considered the optimal C.\n",
            "Finally, the model was retrained using this optimal C value on the entire\n",
            "training dataset and evaluated on a separate, held-out test set to get\n",
            "a final, unbiased estimate of its performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "# make predictions.\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib # Import joblib for saving/loading models\n",
        "import os     # Import os for managing file paths\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the Logistic Regression model\n",
        "print(\"--- Training Logistic Regression Model ---\")\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Evaluate the trained model (optional, but good practice)\n",
        "y_pred_trained = model.predict(X_test)\n",
        "accuracy_trained = accuracy_score(y_test, y_pred_trained)\n",
        "print(f\"Accuracy of the trained model: {accuracy_trained:.4f}\")\n",
        "\n",
        "\n",
        "# Step 4: Save the trained model to a file using joblib\n",
        "# Define the filename for the saved model\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "\n",
        "print(f\"\\n--- Saving the trained model to '{model_filename}' ---\")\n",
        "try:\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(\"Model saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "\n",
        "# Step 5: Load the model from the file using joblib\n",
        "print(f\"\\n--- Loading the model from '{model_filename}' ---\")\n",
        "loaded_model = None # Initialize loaded_model\n",
        "if os.path.exists(model_filename):\n",
        "    try:\n",
        "        loaded_model = joblib.load(model_filename)\n",
        "        print(\"Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "else:\n",
        "    print(f\"Error: Model file '{model_filename}' not found.\")\n",
        "\n",
        "\n",
        "# Step 6: Make predictions using the loaded model\n",
        "if loaded_model is not None:\n",
        "    print(\"\\n--- Making predictions using the loaded model ---\")\n",
        "    # Use the loaded model to make predictions on the test data\n",
        "    y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "    # Evaluate the predictions from the loaded model\n",
        "    accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "\n",
        "    print(f\"Accuracy of the loaded model's predictions: {accuracy_loaded:.4f}\")\n",
        "\n",
        "    # Verify that predictions from the original and loaded models are the same\n",
        "    if np.array_equal(y_pred_trained, y_pred_loaded):\n",
        "        print(\"Predictions from original and loaded models match.\")\n",
        "    else:\n",
        "        print(\"Predictions from original and loaded models DO NOT match.\")\n",
        "        # This would be unexpected and might indicate an issue with saving/loading.\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not load the model, skipping prediction step.\")\n",
        "\n",
        "# Clean up the saved model file (optional)\n",
        "# print(f\"\\n--- Cleaning up: Removing '{model_filename}' ---\")\n",
        "# if os.path.exists(model_filename):\n",
        "#     os.remove(model_filename)\n",
        "#     print(\"Model file removed.\")\n",
        "# else:\n",
        "#     print(\"Model file not found for removal.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Explanation ---\")\n",
        "print(\"We trained a Logistic Regression model, saved it to a file using joblib,\")\n",
        "print(\"and then loaded the model back into memory. Finally, we used the loaded\")\n",
        "print(\"model to make predictions on the test set and verified that it produces\")\n",
        "print(\"the same results as the original trained model.\")\n",
        "print(\"This process is essential for deploying machine learning models without\")\n",
        "print(\"retraining them every time they are needed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9q8tTv6mzPN",
        "outputId": "7c571d45-4e8d-4398-d11c-29163e167616"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Logistic Regression Model ---\n",
            "Model training complete.\n",
            "Accuracy of the trained model: 1.0000\n",
            "\n",
            "--- Saving the trained model to 'logistic_regression_model.joblib' ---\n",
            "Model saved successfully.\n",
            "\n",
            "--- Loading the model from 'logistic_regression_model.joblib' ---\n",
            "Model loaded successfully.\n",
            "\n",
            "--- Making predictions using the loaded model ---\n",
            "Accuracy of the loaded model's predictions: 1.0000\n",
            "Predictions from original and loaded models match.\n",
            "\n",
            "--- Explanation ---\n",
            "We trained a Logistic Regression model, saved it to a file using joblib,\n",
            "and then loaded the model back into memory. Finally, we used the loaded\n",
            "model to make predictions on the test set and verified that it produces\n",
            "the same results as the original trained model.\n",
            "This process is essential for deploying machine learning models without\n",
            "retraining them every time they are needed.\n"
          ]
        }
      ]
    }
  ]
}